---
title: "Basic & Econometrics - OLS via Stochastic Gradient Descent"
header:
  teaser: /assets/images/notebooks/BigDataEconometrics/OLS/output_4_0.png
toc: true
toc_sticky: true
toc_label: Contents
category: "Big Data Econometrics" 
---

```python
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from random import seed, random, gauss
import statsmodels.api as sm

```

    /Users/manguito/.virtualenvs/myEnv/lib/python3.7/site-packages/statsmodels/compat/pandas.py:49: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
      data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)
    

### OLS with Stochastic Gradient Descent


```python
# Generate data
n = 100
np.random.seed(0)
X = 2.5 * np.random.randn(n) + 1.5   # Array of n values with mean = 1.5, stddev = 2.5
res = 0.5 * np.random.randn(n)       # Generate n residual terms
y = 2 + 0.3 * X + res                  # Actual values of Y

# Create pandas dataframe to store our X and y values
df = pd.DataFrame(
    {'X': X,
     'y': y}
)

# Show the first five rows of our dataframe
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.910131</td>
      <td>4.714615</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.500393</td>
      <td>2.076238</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.946845</td>
      <td>2.548811</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.102233</td>
      <td>4.615368</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6.168895</td>
      <td>3.264107</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Calculate the mean of X and y
xmean = np.mean(X)
ymean = np.mean(y)

# Calculate the terms needed for the numator and denominator of beta
df['xycov'] = (df['X'] - xmean) * (df['y'] - ymean)
df['xvar'] = (df['X'] - xmean)**2

# Calculate beta and alpha
beta = df['xycov'].sum() / df['xvar'].sum()
alpha = ymean - (beta * xmean)
print(f'alpha = {alpha}')
print(f'beta = {beta}')
```

    alpha = 2.0031670124623426
    beta = 0.3229396867092763
    


```python
ypred = alpha + beta * X

# Plot regression against actual data
plt.figure(figsize=(12, 6))
plt.plot(X, ypred)     # regression line
plt.plot(X, y, 'ro')   # scatter plot showing actual data
plt.title('Actual vs Predicted')
plt.xlabel('X')
plt.ylabel('y')

plt.show()
```


    
![png](/assets/images/notebooks/BigDataEconometrics/OLS/output_4_0.png)
    



```python
# Make a prediction with coefficients
def predict(row, coefficients):
	yhat = coefficients[0]
	for i in range(len(row)-1):
		yhat += coefficients[i + 1] * row[i]
	return yhat
 
# Estimate linear regression coefficients using stochastic gradient descent
def coefficients_sgd(train, l_rate, n_epoch):
	coef = [0.0 for i in range(len(train[0]))]
	for epoch in range(n_epoch):
		sum_error = 0
		for row in train:
			yhat = predict(row, coef)
			error = yhat - row[-1]
			sum_error += error**2
			coef[0] = coef[0] - l_rate * error
			for i in range(len(row)-1):
				coef[i + 1] = coef[i + 1] - l_rate * error * row[i]
		print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
	return coef
 
# # Calculate coefficients
# dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]
dataset = np.array(df[['X','y']])
l_rate = 0.001
n_epoch = 50
coef = coefficients_sgd(dataset, l_rate, n_epoch)
print(coef)
# df.head()
```

    >epoch=0, lrate=0.001, error=468.733
    >epoch=1, lrate=0.001, error=258.425
    >epoch=2, lrate=0.001, error=206.738
    >epoch=3, lrate=0.001, error=180.854
    >epoch=4, lrate=0.001, error=160.966
    >epoch=5, lrate=0.001, error=143.946
    >epoch=6, lrate=0.001, error=129.113
    >epoch=7, lrate=0.001, error=116.156
    >epoch=8, lrate=0.001, error=104.837
    >epoch=9, lrate=0.001, error=94.950
    >epoch=10, lrate=0.001, error=86.314
    >epoch=11, lrate=0.001, error=78.772
    >epoch=12, lrate=0.001, error=72.184
    >epoch=13, lrate=0.001, error=66.430
    >epoch=14, lrate=0.001, error=61.404
    >epoch=15, lrate=0.001, error=57.015
    >epoch=16, lrate=0.001, error=53.181
    >epoch=17, lrate=0.001, error=49.832
    >epoch=18, lrate=0.001, error=46.907
    >epoch=19, lrate=0.001, error=44.352
    >epoch=20, lrate=0.001, error=42.120
    >epoch=21, lrate=0.001, error=40.171
    >epoch=22, lrate=0.001, error=38.468
    >epoch=23, lrate=0.001, error=36.980
    >epoch=24, lrate=0.001, error=35.681
    >epoch=25, lrate=0.001, error=34.545
    >epoch=26, lrate=0.001, error=33.554
    >epoch=27, lrate=0.001, error=32.687
    >epoch=28, lrate=0.001, error=31.930
    >epoch=29, lrate=0.001, error=31.269
    >epoch=30, lrate=0.001, error=30.691
    >epoch=31, lrate=0.001, error=30.186
    >epoch=32, lrate=0.001, error=29.745
    >epoch=33, lrate=0.001, error=29.359
    >epoch=34, lrate=0.001, error=29.022
    >epoch=35, lrate=0.001, error=28.728
    >epoch=36, lrate=0.001, error=28.471
    >epoch=37, lrate=0.001, error=28.246
    >epoch=38, lrate=0.001, error=28.049
    >epoch=39, lrate=0.001, error=27.877
    >epoch=40, lrate=0.001, error=27.727
    >epoch=41, lrate=0.001, error=27.596
    >epoch=42, lrate=0.001, error=27.481
    >epoch=43, lrate=0.001, error=27.381
    >epoch=44, lrate=0.001, error=27.293
    >epoch=45, lrate=0.001, error=27.216
    >epoch=46, lrate=0.001, error=27.149
    >epoch=47, lrate=0.001, error=27.090
    >epoch=48, lrate=0.001, error=27.039
    >epoch=49, lrate=0.001, error=26.994
    [1.9381705148304995, 0.335948469233473]
    

Compare to coefficient estimates from statsmodels


```python
mod = sm.OLS(df.y,sm.add_constant(df['X']))
res = mod.fit()
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.715</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.712</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   245.5</td>
</tr>
<tr>
  <th>Date:</th>             <td>Thu, 10 Dec 2020</td> <th>  Prob (F-statistic):</th> <td>1.93e-28</td>
</tr>
<tr>
  <th>Time:</th>                 <td>15:28:41</td>     <th>  Log-Likelihood:    </th> <td> -75.359</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   154.7</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   159.9</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    2.0032</td> <td>    0.062</td> <td>   32.273</td> <td> 0.000</td> <td>    1.880</td> <td>    2.126</td>
</tr>
<tr>
  <th>X</th>     <td>    0.3229</td> <td>    0.021</td> <td>   15.669</td> <td> 0.000</td> <td>    0.282</td> <td>    0.364</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 5.184</td> <th>  Durbin-Watson:     </th> <td>   1.995</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.075</td> <th>  Jarque-Bera (JB):  </th> <td>   3.000</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.210</td> <th>  Prob(JB):          </th> <td>   0.223</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.262</td> <th>  Cond. No.          </th> <td>    3.73</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.




```python

```
