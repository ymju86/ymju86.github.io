---
title: "Causal Inference - Differences-in-Differences"
header:
  teaser: /assets/images/notebooks/Causal_Inference/output_67_1.png
toc: true
toc_sticky: true
toc_label: Contents
category: "Big Data Econometrics" 
---

```python
import pandas as pd
import numpy as np
import random
import statsmodels.api as sm
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
import matplotlib.pyplot as plt
from tqdm import tqdm
import statsmodels.api as sm
import itertools

import copy
random.seed(10)
```


```python
img_dir = '/Users/ida/Google Drive/Big Data Econometrics/Slides/images/'
```


```python
def fn_generate_cov(dim,corr):
    acc  = []
    for i in range(dim):
        row = np.ones((1,dim)) * corr
        row[0][i] = 1
        acc.append(row)
    return np.concatenate(acc,axis=0)

def fn_generate_multnorm(nobs,corr,nvar):

    mu = np.zeros(nvar)
    std = (np.abs(np.random.normal(loc = 1, scale = .5,size = (nvar,1))))**(1/2)
    # generate random normal distribution
    acc = []
    for i in range(nvar):
        acc.append(np.reshape(np.random.normal(mu[i],std[i],nobs),(nobs,-1)))
    
    normvars = np.concatenate(acc,axis=1)

    cov = fn_generate_cov(nvar,corr)
    C = np.linalg.cholesky(cov)

    Y = np.transpose(np.dot(C,np.transpose(normvars)))

#     return (Y,np.round(np.corrcoef(Y,rowvar=False),2))
    return Y

def fn_randomize_treatment(N,p=0.5):
    treated = random.sample(range(N), round(N*p))
    return np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])

def fn_bias_rmse_size(theta0,thetahat,se_thetahat,cval = 1.96):
    b = thetahat - theta0
    bias = np.mean(b)
    rmse = np.sqrt(np.mean(b**2))
    tval = b/se_thetahat
    size = np.mean(1*(np.abs(tval)>cval))
    # note size calculated at true parameter value
    return (bias,rmse,size)


def fn_plot_with_ci(n_values,tauhats,tau,lb,ub,caption):
    fig = plt.figure(figsize = (10,6))
    plt.plot(n_values,tauhats,label = '$\hat{\\tau}$')
    plt.xlabel('N')
    plt.ylabel('$\hat{\\tau}$')
    plt.axhline(y=tau, color='r', linestyle='-',linewidth=1,
                label='True $\\tau$={}'.format(tau))
    plt.title('{}'.format(caption))
    plt.fill_between(n_values, lb, ub,
        alpha=0.5, edgecolor='#FF9848', facecolor='#FF9848',label = '95% CI')
    plt.legend()
    
def fn_group_to_ind(vecG,n_g):
    """
    Transform group effects vector of length G into an n-length individual effects vector 
    """
#     g = len(vecG)
#     return np.concatenate([np.concatenate([vecG[g] for i in range(n_g)]) for g in range(G)]).\
#     reshape([n,1])
    return np.array(list(itertools.chain.from_iterable(itertools.repeat(x,n_g) for x in vecG)))

def fn_ind_to_panel(vec,T):
    """
    Transform (n x 1) vector of individual specific effect to an (n x T) matrix
    """
    return np.concatenate([vec for i in range(T)],axis =1)

def fn_mat_wide_to_long(mat,n,T):
    """
    Take (n x T) matrix and output nxT vector with observations for each t stacked on top
    """
    return np.concatenate([mat[:,i] for i in range(T)]).reshape([n*T,1])

def fn_create_wide_to_long_df(data,colnames,n,G,T):
    """
    Take list of matrices in wide format and output a dataframe in long format
    """
    n_g = n/G # number of observations in each group
    if n_g.is_integer()==False:
        print('Error: n_g is not an integer')
    else:
        n_g = int(n_g)
    group = np.concatenate([fn_group_to_ind(np.array(range(G)).reshape([G,1]),n_g) for i in range(T)])
    if len(data)!=len(colnames):
        print('Error: number of column names not equal to number of variables')
    dataDict = {}
    for i in range(len(colnames)):
        dataDict[colnames[i]] = fn_mat_wide_to_long(data[i],n,T)[:,0]

    dataDict['group'] = group[:,0]+1
    dataDict['n'] = 1+np.concatenate([range(n) for i in range(T)])
    dataDict['t'] = 1+np.array(list(itertools.chain.from_iterable(itertools.repeat(i,n) for i in range(T))))
    return pd.DataFrame(dataDict)

# def fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend = False):
#     n_g = n/G # number of observations in each group
#     if n_g.is_integer()==False:
#         print('Error: n_g is not an integer')
#     else:
#         n_g = int(n_g)

#     alphaG = np.random.normal(1,1,[G,1]) # (G x 1) group fixed effects
#     alpha_i = fn_group_to_ind(alphaG,n_g) # (n x 1)
#     treatG = np.zeros([G,1])
#     treatG[:n_treated,] = 1
#     treat_i = fn_group_to_ind(treatG,n_g) # (n x 1)
#     # convert to (n X T)
#     if linear_trend==True:
#         gamma = np.vstack([np.array(range(T)) for i in range(n)])
#     else:
#         gamma = np.ones([n,1])*np.random.normal(0,1,[1,T]) # (n x T) # time specific effects   
#     alpha = fn_ind_to_panel(alpha_i,T)
#     treat = fn_ind_to_panel(treat_i,T)
#     treat = np.concatenate([0*treat[:,:treat_start],treat[:,treat_start:]],axis = 1)
    
#     sig = (np.random.chisquare(2,[n,1])/4+0.5)**(1/2)
#     eps = sig*((np.random.chisquare(2,[n,T])-2)*0.5) # (n x T)
#     X = np.concatenate([fn_generate_multnorm(n,.5,1) for i in range(T)],axis =1) # (n x T)
#     Y = alpha + gamma + beta*treat + delta*X+eps
    
#     return [Y,alpha,gamma,treat,X]

def fn_plot_dd(dfg,treat_start,fig_name=False):
    """
    Plot average outcome Y by group
    """
    
    Yg = dfg[['Y','I','group','t']].groupby(['group','t']).mean().reset_index()
    treatStatus = dict(zip(Yg[Yg.t==Yg.t.max()]['group'],Yg[Yg.t==Yg.t.max()]['I']))
    fig = plt.figure(figsize = (10,6))
    for g in Yg.group.unique():
        plt.plot(Yg[Yg.group==g]['t'],Yg[Yg.group==g]['Y'],label = 'treatment={}'.format(int(treatStatus[g])))
    plt.axvline(x=treat_start+1,color = 'red')
    plt.xlabel('time period')
    plt.ylabel('outcome')
    plt.legend()
    if fig_name:
        plt.savefig(img_dir + 'dd1.png')
        
def fn_within_transformation(dfg,varlist,group_var):
    """
    Transform each variable in the varlist using the within transformation to eliminate
    the group-fixed effects
    """
    dfm = dfg[varlist+[group_var]].groupby([group_var]).mean().\
                reset_index().\
                rename(columns = {k:'{}_bar'.format(k) for k in varlist})
    dfg = dfg.merge(dfm, on = ['group'],how = 'left')
    dfg['const'] = 1
    for v in varlist:
        dfg['{}_w'.format(v)] = dfg[v] - dfg['{}_bar'.format(v)]
    return dfg
```

Statsmodels sandwich variance estimators
https://github.com/statsmodels/statsmodels/blob/master/statsmodels/stats/sandwich_covariance.py

## 1. Generate data

$Y_{it} = W_{it}\tau_{it}+\mu_i+\delta_t+\varepsilon_{it}$

$\varepsilon_{it}^{\left(  r\right)  }/\sigma_{i}\sim IID\left[\chi^{2}(2)-2\right]  /2$

$\sigma_{i}^{2}\sim IID\left[  \chi^{2}(2)/4+0.5\right]$ 


```python
# def fn_randomize_treatment(N,p=0.5):
N = 11
p = .5
treated = random.sample(range(N), round(N*p))
# 

treated


```




    [9, 0, 6, 7, 4, 10]




```python
l1 = list(range(10))
l1
```




    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]




```python
%%timeit
# add 2 to every element
l2 = []
for el in range(len(l1)):
    l2 = l2 + [el+2]
l2
```

    1.23 µs ± 90.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
    


```python
%%timeit
[(i+2) for i in l1]
```

    525 ns ± 5.34 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
    


```python
np.array([(1 if i in treated else 0) for i in range(N)]).reshape([N,1])
```




    array([[1],
           [0],
           [0],
           [0],
           [1],
           [0],
           [1],
           [1],
           [0],
           [1],
           [1]])




```python
n = 1000
T = 2
t_treat = 2# first treatment period
p = 0.5
tau = 2
mu = np.random.normal(1,1,[n,1]) # (n x 1)
sig = (np.random.chisquare(2,[n,1])/4+0.5)**(1/2)
eps = sig*((np.random.chisquare(2,[n,T])-2)*0.5) # (n x T)
# eps = np.random.normal([n,T])
delta = np.ones([n,1])*np.random.normal(0,1,[1,T]) # (n x T)
treat = fn_randomize_treatment(n,p) # (n x 1)
W = np.concatenate([np.zeros([n,t_treat-1]),treat*np.ones([n,T-t_treat+1])],axis = 1)
Y = W*tau+mu+delta+eps
Y.shape
```




    (1000, 2)




```python
# constant, treatment dummy, time period = 2 dummy , interaction of those 2 dummies
vecY = np.concatenate([Y[:,0],Y[:,1]]).reshape(2000,1)
vecW = np.concatenate([W[:,0],W[:,1]]).reshape(2000,1)
vecT = np.concatenate([np.zeros([n,1]),np.ones([n,1])])
vecInt = vecW*vecT
vecConst = np.ones([2000,1])xvars = np.hstack([vecConst,vecW,vecT,vecInt])
mod = sm.OLS(vecY,xvars)
res = mod.fit()
print(res.summary())
```


```python

```




    (2000, 4)




```python

```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.196
    Model:                            OLS   Adj. R-squared:                  0.195
    Method:                 Least Squares   F-statistic:                     242.9
    Date:                Wed, 17 Feb 2021   Prob (F-statistic):           3.68e-95
    Time:                        16:53:15   Log-Likelihood:                -3549.5
    No. Observations:                2000   AIC:                             7105.
    Df Residuals:                    1997   BIC:                             7122.
    Df Model:                           2                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const          0.6274      0.045     13.889      0.000       0.539       0.716
    x1             0.9360      0.045     20.723      0.000       0.847       1.025
    x2            -0.4562      0.078     -5.831      0.000      -0.610      -0.303
    x3             0.9360      0.045     20.723      0.000       0.847       1.025
    ==============================================================================
    Omnibus:                      287.827   Durbin-Watson:                   1.943
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              671.668
    Skew:                           0.819   Prob(JB):                    1.41e-146
    Kurtosis:                       5.319   Cond. No.                     9.13e+15
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The smallest eigenvalue is 3.7e-29. This might indicate that there are
    strong multicollinearity problems or that the design matrix is singular.
    


```python
Yt
```




    array([[2.07832488, 1.68320036],
           [3.45006643, 1.64195153],
           [3.85554136, 2.00972155],
           [3.49435427, 1.76502504],
           [2.33965988, 0.87416435]])




```python

Yt = Y[np.where(treat==1)[0],:] #select rows where the corresponding row in treat = 1
Yc = Y[np.where(treat==0)[0],:]
Yt.shape, Yc.shape
```




    ((500, 2), (500, 2))




```python
plt.hist(eps)
plt.title('Distribution of errors')
```




    Text(0.5, 1.0, 'Distribution of errors')




    
![png](/assets/images/notebooks/Causal_Inference/output_16_1.png)
    


Estimate parameter of interest by running a regression of $\Delta Y_{i1}$ on the treatment indicator and an intercept


```python
Ydiff = Y[:,1]-Y[:,0]
Wmod = copy.deepcopy(W)
Wmod[:,0] = 1
model = sm.OLS(Ydiff,Wmod)
```


```python
res = model.fit()
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.328</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.327</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   487.3</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 08 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>2.96e-88</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:40:23</td>     <th>  Log-Likelihood:    </th> <td> -1780.2</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   3564.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   998</td>      <th>  BIC:               </th> <td>   3574.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>   -0.5229</td> <td>    0.064</td> <td>   -8.140</td> <td> 0.000</td> <td>   -0.649</td> <td>   -0.397</td>
</tr>
<tr>
  <th>x1</th>    <td>    2.0057</td> <td>    0.091</td> <td>   22.075</td> <td> 0.000</td> <td>    1.827</td> <td>    2.184</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>138.012</td> <th>  Durbin-Watson:     </th> <td>   1.955</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 959.762</td> 
</tr>
<tr>
  <th>Skew:</th>          <td> 0.403</td>  <th>  Prob(JB):          </th> <td>3.89e-209</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 7.731</td>  <th>  Cond. No.          </th> <td>    2.62</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



$Y_{i1} - Y_{i0} = \mu_i + ....... - (\mu_i + .......)$


```python
Yt_diff = np.mean(Yt[:,1])-np.mean(Yt[:,0]) # eliminates mu_i
Yc_diff = np.mean(Yc[:,1])-np.mean(Yc[:,0]) # eliminates mu_i
Yt_diff - Yc_diff # eliminates the delta_t
```




    2.0056559437520955




```python
v_Y = np.concatenate([Y[:,i] for i in range(T)]).reshape([2*n,1])
# v_W = np.concatenate([W[:,i] for i in range(T)]).reshape([2*n,1])
v_W = np.concatenate([W[:,1] for i in range(T)]).reshape([2*n,1])
v_mu = np.concatenate([mu for i in range(T)])
v_n = np.concatenate([range(1,n+1) for i in range(T)]).reshape([2*n,1])
v_t = np.concatenate([i*np.ones([n,1]) for i in range(1,T+1)])
```


```python
df = pd.DataFrame({'y':v_Y[:,0],
                   'W':v_W[:,0],
                   'mu':v_mu[:,0],
                   'n':v_n[:,0],
                   't':v_t[:,0]})
```

Now let $T_i = 1$ if $t=2$ and $T_i=0$ otherwise and run regression

$Y_i = \alpha+W_{i1}+T_i+W_{i1}*T_i$


```python
df['Ti'] = 1*(df.t==2)
df['const'] = 1
df['int'] = df.W*df.Ti
```


```python
mod = sm.OLS(df.y,df[['const','W','Ti','int']])
res = mod.fit()
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.212</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.211</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   179.4</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 08 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>5.26e-103</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:40:24</td>     <th>  Log-Likelihood:    </th> <td> -3550.4</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2000</td>      <th>  AIC:               </th> <td>   7109.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1996</td>      <th>  BIC:               </th> <td>   7131.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    0.6379</td> <td>    0.064</td> <td>    9.978</td> <td> 0.000</td> <td>    0.512</td> <td>    0.763</td>
</tr>
<tr>
  <th>W</th>     <td>   -0.0210</td> <td>    0.090</td> <td>   -0.232</td> <td> 0.816</td> <td>   -0.198</td> <td>    0.156</td>
</tr>
<tr>
  <th>Ti</th>    <td>   -0.5229</td> <td>    0.090</td> <td>   -5.785</td> <td> 0.000</td> <td>   -0.700</td> <td>   -0.346</td>
</tr>
<tr>
  <th>int</th>   <td>    2.0057</td> <td>    0.128</td> <td>   15.687</td> <td> 0.000</td> <td>    1.755</td> <td>    2.256</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>295.177</td> <th>  Durbin-Watson:     </th> <td>   1.945</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 702.857</td> 
</tr>
<tr>
  <th>Skew:</th>          <td> 0.831</td>  <th>  Prob(JB):          </th> <td>2.38e-153</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 5.381</td>  <th>  Cond. No.          </th> <td>    6.85</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



## Let's focus on the more interesting case where we observe multiple time periods and multiple groups

$Y_{igt} = \alpha_g+\gamma_t+\beta I_{gt}+\delta X_{igt}+\varepsilon_{igt}$

Assessing DD identification the key identifying assumption in DD models is that the treatment states/groups (g) have similar trends to the control states in the absence of treatment


```python
def fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend = False,corr = False):
    n_g = n/G # number of observations in each group
    if n_g.is_integer()==False:
        return print('Error: n_g is not an integer')

    else:
        n_g = int(n_g)

        alphaG = np.random.normal(1,1,[G,1]) # (G x 1) group fixed effects
        alpha_i = fn_group_to_ind(alphaG,n_g) # (n x 1)
        treatG = np.zeros([G,1])
        treatG[:n_treated,] = 1
        treat_i = fn_group_to_ind(treatG,n_g) # (n x 1)
        # convert to (n X T)
        if linear_trend==True:
            gamma = np.vstack([np.array(range(T)) for i in range(n)])
        else:
            gamma = np.ones([n,1])*np.random.normal(0,1,[1,T]) # (n x T) # time specific effects   
        alpha = fn_ind_to_panel(alpha_i,T)
        treat = fn_ind_to_panel(treat_i,T)
        treat = np.concatenate([0*treat[:,:treat_start],treat[:,treat_start:]],axis = 1)

        sig = (np.random.chisquare(2,[n,1])/4+0.5)**(1/2)
        eps = sig*((np.random.chisquare(2,[n,T])-2)*0.5) # (n x T)
        
        if corr == True:
            rho = .7
            u = np.zeros(eps.shape)
            u[:,0] = eps[:,0]
            for t in range(1,T):
                u[:,t] = rho*u[:,t-1]+eps[:,t]
                
        X = np.concatenate([fn_generate_multnorm(n,.5,1) for i in range(T)],axis =1) # (n x T)
        if corr == False:
            Y = alpha + gamma + beta*treat + delta*X+eps
        else:
            Y = alpha + gamma + beta*treat + delta*X + u

        return [Y,alpha,gamma,treat,X]
```


```python
# def fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend = False,corr = False):

n = 10
G = 2
T = 5
n_treated = 1
treat_start = 4
beta = 1
delta = .5
linear_trend = True

n_g = n/G # number of observations in each group
if n_g.is_integer()==False:
    print('Error: n_g is not an integer')

```


```python
n_g = int(n_g)

alphaG = np.random.normal(1,1,[G,1]) # (G x 1) group fixed effects
alpha_i = fn_group_to_ind(alphaG,n_g) # (n x 1)
```


```python
treatG = np.zeros([G,1])
treatG[:n_treated,] = 1
treat_i = fn_group_to_ind(treatG,n_g) # (n x 1)
```


```python
treat_i,treatG
```




    (array([[1.],
            [1.],
            [1.],
            [1.],
            [1.],
            [0.],
            [0.],
            [0.],
            [0.],
            [0.]]),
     array([[1.],
            [0.]]))




```python
gamma = np.ones([n,1])*np.random.normal(0,1,[1,T])
gamma
```




    array([[ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079],
           [ 0.53272734,  0.51828703, -0.48633918, -1.74128078, -2.98146079]])



Serially corellated errors:

$u_t = \rho*u_{t-1}+e_t$


```python

# convert to (n X T)
if linear_trend==True:
    gamma = np.vstack([np.array(range(T)) for i in range(n)])
else:
    gamma = np.ones([n,1])*np.random.normal(0,1,[1,T]) # (n x T) # time specific effects   
alpha = fn_ind_to_panel(alpha_i,T)
treat = fn_ind_to_panel(treat_i,T)
treat = np.concatenate([0*treat[:,:treat_start],treat[:,treat_start:]],axis = 1)

sig = (np.random.chisquare(2,[n,1])/4+0.5)**(1/2)
eps = sig*((np.random.chisquare(2,[n,T])-2)*0.5) # (n x T)

if corr == True:
    rho = .7
    u = np.zeros(eps.shape)
    u[:,0] = eps[:,0]
    for t in range(1,T):
        u[:,t] = rho*u[:,t-1]+eps[:,t]
X = np.concatenate([fn_generate_multnorm(n,.5,1) for i in range(T)],axis =1) # (n x T)


```


```python
alpha.shape, gamma.shape, treat.shape
```




    ((10, 5), (10, 5), (10, 5))




```python
treat
```




    array([[0., 0., 0., 0., 1.],
           [0., 0., 0., 0., 1.],
           [0., 0., 0., 0., 1.],
           [0., 0., 0., 0., 1.],
           [0., 0., 0., 0., 1.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]])




```python





if corr == False:
    Y = alpha + gamma + beta*treat + delta*X+eps
else:
    Y = alpha + gamma + beta*treat + delta*X + u

return [Y,alpha,gamma,treat,X]
```


      File "<ipython-input-207-64a684190739>", line 6
        return [Y,alpha,gamma,treat,X]
        ^
    SyntaxError: 'return' outside function
    


$Y_{igt} = \alpha_g+\gamma_t+\beta I_{gt}+\delta X_{igt}+\varepsilon_{igt}$

Within transformation to get rid of $\alpha_g$:

$Y_{igt} - \bar{Y}_{gt}$




```python
n = 2 # 2 units and 1 unit is in each group
G = 2
n_treated = 1
T = 20

beta = 20
delta = 1
treat_start = 10 # when treatment starts
```

Plot the outcome for the treatment and control group


```python
linear_trend = True
[Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend)
```


```python
colnames = ['Y','alpha','gamma','I','X']
data = [Y,alpha,gamma,treat,X]

dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
fn_plot_dd(dfg,treat_start)
```


    
![png](/assets/images/notebooks/Causal_Inference/output_43_0.png)
    



```python
linear_trend = False
[Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend)
colnames = ['Y','alpha','gamma','I','X']
data = [Y,alpha,gamma,treat,X]

dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
fn_plot_dd(dfg,treat_start)
```


    
![png](/assets/images/notebooks/Causal_Inference/output_44_0.png)
    


### Estimation


```python
data = [Y,alpha,gamma,treat,X]
colnames = ['Y','alpha','gamma','I','X']

dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)

dfg = fn_within_transformation(dfg,colnames,'group')   
```


```python
dfg.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Y</th>
      <th>alpha</th>
      <th>gamma</th>
      <th>I</th>
      <th>X</th>
      <th>group</th>
      <th>n</th>
      <th>t</th>
      <th>Y_bar</th>
      <th>alpha_bar</th>
      <th>gamma_bar</th>
      <th>I_bar</th>
      <th>X_bar</th>
      <th>const</th>
      <th>Y_w</th>
      <th>alpha_w</th>
      <th>gamma_w</th>
      <th>I_w</th>
      <th>X_w</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.319496</td>
      <td>-0.895547</td>
      <td>-0.546378</td>
      <td>0.0</td>
      <td>1.377812</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>8.615999</td>
      <td>-0.895547</td>
      <td>-0.162203</td>
      <td>0.5</td>
      <td>-0.15752</td>
      <td>1</td>
      <td>-8.296503</td>
      <td>-2.220446e-16</td>
      <td>-0.384175</td>
      <td>-0.5</td>
      <td>1.535332</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-2.919256</td>
      <td>-0.286708</td>
      <td>-0.546378</td>
      <td>0.0</td>
      <td>-1.078144</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>-0.720491</td>
      <td>-0.286708</td>
      <td>-0.162203</td>
      <td>0.0</td>
      <td>-0.04039</td>
      <td>1</td>
      <td>-2.198765</td>
      <td>-1.110223e-16</td>
      <td>-0.384175</td>
      <td>0.0</td>
      <td>-1.037754</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-2.500770</td>
      <td>-0.895547</td>
      <td>-0.513985</td>
      <td>0.0</td>
      <td>0.332036</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>8.615999</td>
      <td>-0.895547</td>
      <td>-0.162203</td>
      <td>0.5</td>
      <td>-0.15752</td>
      <td>1</td>
      <td>-11.116769</td>
      <td>-2.220446e-16</td>
      <td>-0.351782</td>
      <td>-0.5</td>
      <td>0.489555</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.600016</td>
      <td>-0.286708</td>
      <td>-0.513985</td>
      <td>0.0</td>
      <td>1.036672</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>-0.720491</td>
      <td>-0.286708</td>
      <td>-0.162203</td>
      <td>0.0</td>
      <td>-0.04039</td>
      <td>1</td>
      <td>0.120474</td>
      <td>-1.110223e-16</td>
      <td>-0.351782</td>
      <td>0.0</td>
      <td>1.077062</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.856377</td>
      <td>-0.895547</td>
      <td>1.264813</td>
      <td>0.0</td>
      <td>-0.665249</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>8.615999</td>
      <td>-0.895547</td>
      <td>-0.162203</td>
      <td>0.5</td>
      <td>-0.15752</td>
      <td>1</td>
      <td>-9.472376</td>
      <td>-2.220446e-16</td>
      <td>1.427016</td>
      <td>-0.5</td>
      <td>-0.507730</td>
    </tr>
  </tbody>
</table>
</div>



#### Estimation using a full set of time and group dummies


```python
# pd.get_dummies(dfg.group)
```


```python
xvars = pd.concat([dfg[['I','X']],
                   pd.get_dummies(dfg['group'],drop_first = False),
                   pd.get_dummies(dfg['t'],drop_first = False)],axis = 1)
mod = sm.OLS(dfg['Y'],xvars)
res = mod.fit()
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.992</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.981</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   93.89</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 08 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>1.06e-13</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:40:33</td>     <th>  Log-Likelihood:    </th> <td> -45.914</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    40</td>      <th>  AIC:               </th> <td>   137.8</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    17</td>      <th>  BIC:               </th> <td>   176.7</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    22</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>I</th>  <td>   20.4001</td> <td>    0.767</td> <td>   26.582</td> <td> 0.000</td> <td>   18.781</td> <td>   22.019</td>
</tr>
<tr>
  <th>X</th>  <td>    1.2694</td> <td>    0.277</td> <td>    4.576</td> <td> 0.000</td> <td>    0.684</td> <td>    1.855</td>
</tr>
<tr>
  <th>1</th>  <td>   -1.2907</td> <td>    0.436</td> <td>   -2.959</td> <td> 0.009</td> <td>   -2.211</td> <td>   -0.370</td>
</tr>
<tr>
  <th>2</th>  <td>   -0.5759</td> <td>    0.251</td> <td>   -2.296</td> <td> 0.035</td> <td>   -1.105</td> <td>   -0.047</td>
</tr>
<tr>
  <th>1</th>  <td>   -0.5568</td> <td>    0.824</td> <td>   -0.676</td> <td> 0.508</td> <td>   -2.295</td> <td>    1.182</td>
</tr>
<tr>
  <th>2</th>  <td>   -1.4858</td> <td>    0.841</td> <td>   -1.767</td> <td> 0.095</td> <td>   -3.260</td> <td>    0.288</td>
</tr>
<tr>
  <th>3</th>  <td>    1.1783</td> <td>    0.834</td> <td>    1.412</td> <td> 0.176</td> <td>   -0.582</td> <td>    2.939</td>
</tr>
<tr>
  <th>4</th>  <td>   -0.1733</td> <td>    0.920</td> <td>   -0.189</td> <td> 0.853</td> <td>   -2.113</td> <td>    1.767</td>
</tr>
<tr>
  <th>5</th>  <td>   -1.3141</td> <td>    0.846</td> <td>   -1.554</td> <td> 0.139</td> <td>   -3.099</td> <td>    0.471</td>
</tr>
<tr>
  <th>6</th>  <td>    1.8082</td> <td>    0.845</td> <td>    2.141</td> <td> 0.047</td> <td>    0.026</td> <td>    3.590</td>
</tr>
<tr>
  <th>7</th>  <td>    1.6278</td> <td>    0.825</td> <td>    1.972</td> <td> 0.065</td> <td>   -0.113</td> <td>    3.369</td>
</tr>
<tr>
  <th>8</th>  <td>   -0.1100</td> <td>    0.824</td> <td>   -0.134</td> <td> 0.895</td> <td>   -1.849</td> <td>    1.629</td>
</tr>
<tr>
  <th>9</th>  <td>   -0.6017</td> <td>    0.824</td> <td>   -0.730</td> <td> 0.475</td> <td>   -2.339</td> <td>    1.136</td>
</tr>
<tr>
  <th>10</th> <td>    0.5768</td> <td>    0.824</td> <td>    0.700</td> <td> 0.493</td> <td>   -1.162</td> <td>    2.315</td>
</tr>
<tr>
  <th>11</th> <td>    0.4581</td> <td>    0.836</td> <td>    0.548</td> <td> 0.591</td> <td>   -1.305</td> <td>    2.221</td>
</tr>
<tr>
  <th>12</th> <td>    0.2653</td> <td>    0.839</td> <td>    0.316</td> <td> 0.756</td> <td>   -1.505</td> <td>    2.035</td>
</tr>
<tr>
  <th>13</th> <td>   -0.1689</td> <td>    0.835</td> <td>   -0.202</td> <td> 0.842</td> <td>   -1.930</td> <td>    1.592</td>
</tr>
<tr>
  <th>14</th> <td>    1.6129</td> <td>    0.835</td> <td>    1.932</td> <td> 0.070</td> <td>   -0.148</td> <td>    3.374</td>
</tr>
<tr>
  <th>15</th> <td>   -0.8069</td> <td>    0.848</td> <td>   -0.951</td> <td> 0.355</td> <td>   -2.596</td> <td>    0.983</td>
</tr>
<tr>
  <th>16</th> <td>    1.4182</td> <td>    0.864</td> <td>    1.641</td> <td> 0.119</td> <td>   -0.405</td> <td>    3.242</td>
</tr>
<tr>
  <th>17</th> <td>   -0.6037</td> <td>    0.831</td> <td>   -0.726</td> <td> 0.478</td> <td>   -2.358</td> <td>    1.150</td>
</tr>
<tr>
  <th>18</th> <td>   -2.1873</td> <td>    0.832</td> <td>   -2.628</td> <td> 0.018</td> <td>   -3.943</td> <td>   -0.431</td>
</tr>
<tr>
  <th>19</th> <td>   -1.4659</td> <td>    0.968</td> <td>   -1.514</td> <td> 0.148</td> <td>   -3.509</td> <td>    0.577</td>
</tr>
<tr>
  <th>20</th> <td>   -1.3379</td> <td>    0.840</td> <td>   -1.593</td> <td> 0.130</td> <td>   -3.109</td> <td>    0.434</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 4.798</td> <th>  Durbin-Watson:     </th> <td>   2.930</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.091</td> <th>  Jarque-Bera (JB):  </th> <td>   5.432</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.000</td> <th>  Prob(JB):          </th> <td>  0.0661</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.805</td> <th>  Cond. No.          </th> <td>2.05e+16</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 9.41e-32. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular.



#### Estimation using a within group transformation

_w denotes variables that come out of the within transformation


```python
xvars = pd.concat([dfg[['I_w','X_w']],
                  pd.get_dummies(dfg['t'])],axis = 1)
mod = sm.OLS(dfg['Y_w'],xvars)
res = mod.fit()
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>Y_w</td>       <th>  R-squared:         </th> <td>   0.988</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.975</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   72.02</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 08 Feb 2021</td> <th>  Prob (F-statistic):</th> <td>2.62e-13</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:40:34</td>     <th>  Log-Likelihood:    </th> <td> -45.914</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    40</td>      <th>  AIC:               </th> <td>   135.8</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    18</td>      <th>  BIC:               </th> <td>   173.0</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    21</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>I_w</th> <td>   20.4001</td> <td>    0.746</td> <td>   27.352</td> <td> 0.000</td> <td>   18.833</td> <td>   21.967</td>
</tr>
<tr>
  <th>X_w</th> <td>    1.2694</td> <td>    0.270</td> <td>    4.709</td> <td> 0.000</td> <td>    0.703</td> <td>    1.836</td>
</tr>
<tr>
  <th>1</th>   <td>   -0.4634</td> <td>    0.824</td> <td>   -0.563</td> <td> 0.581</td> <td>   -2.194</td> <td>    1.267</td>
</tr>
<tr>
  <th>2</th>   <td>   -1.3925</td> <td>    0.839</td> <td>   -1.659</td> <td> 0.114</td> <td>   -3.156</td> <td>    0.371</td>
</tr>
<tr>
  <th>3</th>   <td>    1.2716</td> <td>    0.834</td> <td>    1.525</td> <td> 0.145</td> <td>   -0.481</td> <td>    3.024</td>
</tr>
<tr>
  <th>4</th>   <td>   -0.0800</td> <td>    0.915</td> <td>   -0.087</td> <td> 0.931</td> <td>   -2.003</td> <td>    1.843</td>
</tr>
<tr>
  <th>5</th>   <td>   -1.2208</td> <td>    0.844</td> <td>   -1.446</td> <td> 0.165</td> <td>   -2.994</td> <td>    0.553</td>
</tr>
<tr>
  <th>6</th>   <td>    1.9015</td> <td>    0.844</td> <td>    2.253</td> <td> 0.037</td> <td>    0.129</td> <td>    3.674</td>
</tr>
<tr>
  <th>7</th>   <td>    1.7212</td> <td>    0.825</td> <td>    2.086</td> <td> 0.051</td> <td>   -0.012</td> <td>    3.455</td>
</tr>
<tr>
  <th>8</th>   <td>   -0.0167</td> <td>    0.824</td> <td>   -0.020</td> <td> 0.984</td> <td>   -1.748</td> <td>    1.715</td>
</tr>
<tr>
  <th>9</th>   <td>   -0.5083</td> <td>    0.824</td> <td>   -0.617</td> <td> 0.545</td> <td>   -2.239</td> <td>    1.222</td>
</tr>
<tr>
  <th>10</th>  <td>    0.6701</td> <td>    0.824</td> <td>    0.813</td> <td> 0.427</td> <td>   -1.061</td> <td>    2.401</td>
</tr>
<tr>
  <th>11</th>  <td>    0.5514</td> <td>    0.828</td> <td>    0.666</td> <td> 0.514</td> <td>   -1.187</td> <td>    2.290</td>
</tr>
<tr>
  <th>12</th>  <td>    0.3586</td> <td>    0.831</td> <td>    0.431</td> <td> 0.671</td> <td>   -1.388</td> <td>    2.105</td>
</tr>
<tr>
  <th>13</th>  <td>   -0.0755</td> <td>    0.827</td> <td>   -0.091</td> <td> 0.928</td> <td>   -1.813</td> <td>    1.662</td>
</tr>
<tr>
  <th>14</th>  <td>    1.7062</td> <td>    0.827</td> <td>    2.063</td> <td> 0.054</td> <td>   -0.031</td> <td>    3.444</td>
</tr>
<tr>
  <th>15</th>  <td>   -0.7136</td> <td>    0.839</td> <td>   -0.850</td> <td> 0.406</td> <td>   -2.477</td> <td>    1.050</td>
</tr>
<tr>
  <th>16</th>  <td>    1.5116</td> <td>    0.856</td> <td>    1.766</td> <td> 0.094</td> <td>   -0.286</td> <td>    3.310</td>
</tr>
<tr>
  <th>17</th>  <td>   -0.5103</td> <td>    0.824</td> <td>   -0.619</td> <td> 0.543</td> <td>   -2.241</td> <td>    1.220</td>
</tr>
<tr>
  <th>18</th>  <td>   -2.0940</td> <td>    0.825</td> <td>   -2.540</td> <td> 0.021</td> <td>   -3.826</td> <td>   -0.362</td>
</tr>
<tr>
  <th>19</th>  <td>   -1.3726</td> <td>    0.954</td> <td>   -1.439</td> <td> 0.167</td> <td>   -3.376</td> <td>    0.631</td>
</tr>
<tr>
  <th>20</th>  <td>   -1.2445</td> <td>    0.831</td> <td>   -1.497</td> <td> 0.152</td> <td>   -2.991</td> <td>    0.502</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 4.798</td> <th>  Durbin-Watson:     </th> <td>   2.930</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.091</td> <th>  Jarque-Bera (JB):  </th> <td>   5.432</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.000</td> <th>  Prob(JB):          </th> <td>  0.0661</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.805</td> <th>  Cond. No.          </th> <td>    7.06</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.



### Multiple groups and time periods

$Y_{igt} = \alpha_g+\gamma_t+\beta I_{gt}+\delta X_{igt}+\varepsilon_{igt}$



```python
n = 1000
G = 10
n_treated = 5
T = 20

beta = 20
delta = 1
treat_start = 10

```


```python
linear_trend = True
[Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend)
colnames = ['Y','alpha','gamma','I','X']
data = [Y,alpha,gamma,treat,X]
dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
fn_plot_dd(dfg,treat_start)
# dfg.head()
```


    
![png](/assets/images/notebooks/Causal_Inference/output_56_0.png)
    


### Estimation



```python
dfw = fn_within_transformation(dfg,colnames,'group')   
```


```python
xvars = pd.concat([dfw[['I_w','X_w']],
                  pd.get_dummies(dfw['t'])],axis = 1)
mod = sm.OLS(dfw['Y_w'],xvars,cluster= dfw['group'])
res = mod.fit(cov_type='HC1')
t = res.params.I_w/res.HC1_se.I_w
```


```python
res.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>Y_w</td>       <th>  R-squared:         </th> <td>   0.993</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.993</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>     nan</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 08 Feb 2021</td> <th>  Prob (F-statistic):</th>  <td>   nan</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>13:40:36</td>     <th>  Log-Likelihood:    </th> <td> -28192.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td> 20000</td>      <th>  AIC:               </th> <td>5.643e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td> 19978</td>      <th>  BIC:               </th> <td>5.660e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    21</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
   <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>I_w</th> <td>   19.9867</td> <td>    0.028</td> <td>  712.870</td> <td> 0.000</td> <td>   19.932</td> <td>   20.042</td>
</tr>
<tr>
  <th>X_w</th> <td>    0.9981</td> <td>    0.007</td> <td>  147.009</td> <td> 0.000</td> <td>    0.985</td> <td>    1.011</td>
</tr>
<tr>
  <th>1</th>   <td>   -9.5285</td> <td>    0.032</td> <td> -294.481</td> <td> 0.000</td> <td>   -9.592</td> <td>   -9.465</td>
</tr>
<tr>
  <th>2</th>   <td>   -8.5477</td> <td>    0.031</td> <td> -278.041</td> <td> 0.000</td> <td>   -8.608</td> <td>   -8.487</td>
</tr>
<tr>
  <th>3</th>   <td>   -7.4749</td> <td>    0.032</td> <td> -233.314</td> <td> 0.000</td> <td>   -7.538</td> <td>   -7.412</td>
</tr>
<tr>
  <th>4</th>   <td>   -6.5301</td> <td>    0.031</td> <td> -213.574</td> <td> 0.000</td> <td>   -6.590</td> <td>   -6.470</td>
</tr>
<tr>
  <th>5</th>   <td>   -5.5166</td> <td>    0.031</td> <td> -178.651</td> <td> 0.000</td> <td>   -5.577</td> <td>   -5.456</td>
</tr>
<tr>
  <th>6</th>   <td>   -4.4325</td> <td>    0.034</td> <td> -130.609</td> <td> 0.000</td> <td>   -4.499</td> <td>   -4.366</td>
</tr>
<tr>
  <th>7</th>   <td>   -3.5591</td> <td>    0.030</td> <td> -116.873</td> <td> 0.000</td> <td>   -3.619</td> <td>   -3.499</td>
</tr>
<tr>
  <th>8</th>   <td>   -2.5156</td> <td>    0.032</td> <td>  -77.789</td> <td> 0.000</td> <td>   -2.579</td> <td>   -2.452</td>
</tr>
<tr>
  <th>9</th>   <td>   -1.4233</td> <td>    0.034</td> <td>  -41.344</td> <td> 0.000</td> <td>   -1.491</td> <td>   -1.356</td>
</tr>
<tr>
  <th>10</th>  <td>   -0.4743</td> <td>    0.032</td> <td>  -14.866</td> <td> 0.000</td> <td>   -0.537</td> <td>   -0.412</td>
</tr>
<tr>
  <th>11</th>  <td>    0.4865</td> <td>    0.032</td> <td>   15.107</td> <td> 0.000</td> <td>    0.423</td> <td>    0.550</td>
</tr>
<tr>
  <th>12</th>  <td>    1.5096</td> <td>    0.033</td> <td>   46.048</td> <td> 0.000</td> <td>    1.445</td> <td>    1.574</td>
</tr>
<tr>
  <th>13</th>  <td>    2.4724</td> <td>    0.031</td> <td>   78.889</td> <td> 0.000</td> <td>    2.411</td> <td>    2.534</td>
</tr>
<tr>
  <th>14</th>  <td>    3.5285</td> <td>    0.031</td> <td>  112.952</td> <td> 0.000</td> <td>    3.467</td> <td>    3.590</td>
</tr>
<tr>
  <th>15</th>  <td>    4.4621</td> <td>    0.030</td> <td>  148.465</td> <td> 0.000</td> <td>    4.403</td> <td>    4.521</td>
</tr>
<tr>
  <th>16</th>  <td>    5.5001</td> <td>    0.032</td> <td>  170.856</td> <td> 0.000</td> <td>    5.437</td> <td>    5.563</td>
</tr>
<tr>
  <th>17</th>  <td>    6.5112</td> <td>    0.032</td> <td>  204.902</td> <td> 0.000</td> <td>    6.449</td> <td>    6.574</td>
</tr>
<tr>
  <th>18</th>  <td>    7.5147</td> <td>    0.035</td> <td>  215.853</td> <td> 0.000</td> <td>    7.446</td> <td>    7.583</td>
</tr>
<tr>
  <th>19</th>  <td>    8.4975</td> <td>    0.034</td> <td>  246.447</td> <td> 0.000</td> <td>    8.430</td> <td>    8.565</td>
</tr>
<tr>
  <th>20</th>  <td>    9.5200</td> <td>    0.031</td> <td>  308.540</td> <td> 0.000</td> <td>    9.460</td> <td>    9.580</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>9425.670</td> <th>  Durbin-Watson:     </th> <td>   2.007</td> 
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>68273.339</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 2.146</td>  <th>  Prob(JB):          </th> <td>    0.00</td> 
</tr>
<tr>
  <th>Kurtosis:</th>       <td>10.969</td>  <th>  Cond. No.          </th> <td>    7.40</td> 
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity robust (HC1)



### Simulate placebo policy


```python
def fn_placebo_treatment(row,tstart,treated):
    if (row.t>=tstart) & (row.group in treated):
        return 1
    else:
        return 0
```

### THIS IS THE PART WE DIDN'T COVER IN LECTURE 4
What the code below does is to simulate a placebo policy - where we mistakenly treat some units as being assigned a treatment even though they're not and we estiamte the treatment effect
We expect to reject the null hypothesis that the treatment effect is different from 0 a low % of the time because the treatment effect doesn't exist so it is zero!
Indeed, we reject the null around 1% of the time below. With more replications it should converge to 5%.


```python
R = 100
l = []
n = 1000
G = 10
n_treated = 0
T = 20

beta = 20
delta = 1
treat_start = 10
linear_trend = True
colnames = ['Y','alpha','gamma','I','X']

for r in tqdm(range(R)):
    np.random.seed(r)
    [Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend)
    data = [Y,alpha,gamma,treat,X]
    dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
    tstart = random.choice(range(dfg.t.min(),dfg.t.max()+1)) # treatment start date
    treated = random.choices(range(dfg.group.min(),dfg.group.max()+1),k = int(dfg.group.max()/2))
    dfg['I'] = dfg.apply(lambda row: fn_placebo_treatment(row,tstart,treated),axis = 1)
    dfw = fn_within_transformation(dfg,colnames,'group')  
    xvars = pd.concat([dfw[['I_w','X_w']],
                  pd.get_dummies(dfw['t'])],axis = 1)
    mod = sm.OLS(dfw['Y_w'],xvars,cluster= dfw['group'])
    res = mod.fit(cov_type='HC1')
    t = res.params.I_w/res.HC1_se.I_w
    if np.abs(t)>1.96:
        l = l+[1]
    else:
        l = l+[0]
```

      6%|▌         | 6/100 [00:02<00:35,  2.61it/s]<ipython-input-223-02feb1c08ec8>:27: RuntimeWarning: invalid value encountered in double_scalars
      t = res.params.I_w/res.HC1_se.I_w
    100%|██████████| 100/100 [00:32<00:00,  3.11it/s]
    


```python
np.mean(l) # we reject the null of a nonzero treatment effect 1% of the time
```




    0.01



### Now let's add some serial correlation to the DGP


```python
n = 1000
G = 10
n_treated = 5
T = 20

beta = 20
delta = 1
treat_start = 10

linear_trend = True
serial_corr = True
[Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend,serial_corr)
colnames = ['Y','alpha','gamma','I','X']
data = [Y,alpha,gamma,treat,X]
dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
fn_plot_dd(dfg,treat_start)
```


    
![png](/assets/images/notebooks/Causal_Inference/output_67_1.png)
    


$y_{it}$

shock at time $t$ equal to $\gamma_t$

Estimate:

$y_{it}  = \alpha + \beta*\gamma_t+\varepsilon_{it}$

Obtain residuals $\hat{\varepsilon}_{it}$


```python

```


```python

```


```python

```


```python

```


```python
def fn_run_mc(beta,R,corr):
    l0 = []
    l1 = []
    for r in tqdm(range(R)):
        np.random.seed(r)
        [Y,alpha,gamma,treat,X]= fn_generate_grouped_panel(n,G,T,n_treated,treat_start,beta,delta,linear_trend,corr)
        data = [Y,alpha,gamma,treat,X]
        dfg = fn_create_wide_to_long_df(data,colnames,n,G,T)
        tstart = random.choice(range(dfg.t.min(),dfg.t.max()+1))
        treated = random.choices(range(dfg.group.min(),dfg.group.max()+1),k = int(dfg.group.max()/2))
    #     dfg['I'] = dfg.apply(lambda row: fn_placebo_treatment(row,tstart,treated),axis = 1)
        dfw = fn_within_transformation(dfg,colnames,'group')  
        xvars = pd.concat([dfw[['I_w','X_w']],
                      pd.get_dummies(dfw['t'])],axis = 1)
        mod = sm.OLS(dfw['Y_w'],xvars,cluster= dfw['group'])
        res = mod.fit(cov_type='HC1')
        t0 = res.params.I_w/res.HC1_se.I_w # test H0: beta=0
        t1 = (res.params.I_w-beta)/res.HC1_se.I_w # test H0: beta = beta_true

        l0 = l0 + [1*(np.abs(t0)>1.96)]
        l1 = l1 + [1*(np.abs(t1)>1.96)]

    return np.mean(l0),np.mean(l1)
```


```python
t_stat = .2
t_stat>1.96, 1*(t_stat>1.96)
```




    (False, 0)




```python
R = 500
n = 1000
G = 10
n_treated = 5
T = 20
delta = 1
treat_start = 10
linear_trend = True
colnames = ['Y','alpha','gamma','I','X']

```

##### Treatment effect = 2 with no serial correlation


```python
beta = 2
corr = False

h1, hb = fn_run_mc(beta,R,corr)
h1,hb
```

    100%|██████████| 500/500 [00:38<00:00, 13.04it/s]
    




    (1.0, 0.066)



##### Treatment effect = 2 with serial correlation


```python
beta = 2
corr = True

h1, hb = fn_run_mc(beta,R,corr)
h1,hb
```

    100%|██████████| 500/500 [00:25<00:00, 19.81it/s]
    




    (1.0, 0.34)



##### Treatment effect = 0, no serial correlation


```python
beta = 0
corr = False

h1, hb = fn_run_mc(beta,R,corr)
print('We reject beta=0 {} of the time'.format(h1))
```

    100%|██████████| 500/500 [00:27<00:00, 18.02it/s]

    We reject beta=0 0.066% of the time
    

    
    

##### Treatment effect = 0, serial correlation


```python
beta = 0
corr = True

h1, hb = fn_run_mc(beta,R,corr)
print('We reject beta=0 {} of the time'.format(h1))
```

    100%|██████████| 500/500 [00:37<00:00, 13.47it/s]

    We reject beta=0 0.34 of the time
    

    
    

## Sentsitivity tests

Redo the analysis on pre-event years - the estiamted treatment effect should be zero!
Are treatment and control gropus similar along observable dimensions?
Make sure the change is concentrated around the event
Make sure tha tother outcome variables that should be unaffected by the event are indeed unaffected


```python

```


```python

```
