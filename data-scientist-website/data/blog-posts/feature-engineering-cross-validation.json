{
  "metadata": {
    "title": "Feature Engineering and Cross-Validation for Online Shopper Intention Prediction",
    "date": "2021-02-06",
    "author": "Youngmin Ju",
    "excerpt": "A comprehensive guide to feature engineering and cross-validation techniques for predicting online shopper purchase intentions.",
    "thumbnail": "/images/ds4.png",
    "tags": ["Machine Learning", "Feature Engineering", "Cross-Validation", "Classification"]
  },
  "content": "# Online Shoppers Intention Prediction\n\nThis project focuses on predicting whether an online shopping session will end with a purchase based on various session attributes.\n\n## Data Description\n\nThe dataset consists of feature vectors from 12,330 sessions, with 84.5% not ending with shopping and 15.5% ending with shopping. Features include:\n\n- Page visit statistics (Administrative, Informational, Product Related)\n- Duration metrics for different page types\n- Bounce rates and exit rates\n- Page values\n- Special day proximity\n- Visitor information (OS, Browser, Region, Traffic Type)\n- Temporal information (Month, Weekend)\n\n## Exploratory Data Analysis\n\nLet's start by loading and exploring the data:\n\n```python\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\n# Load the data\ndf = pd.read_csv(\"online_shoppers_intention.csv\")\n\n# Display the first few rows\ndf.head()\n```\n\nLet's check the data types and missing values:\n\n```python\n# Check data types\ndf.info()\n\n# Check for missing values\nprint(df.isnull().sum())\n```\n\nLet's transform boolean columns to binary:\n\n```python\n# Transform boolean columns to binary\ndf.Revenue = df.Revenue.astype('int')\ndf.Weekend = df.Weekend.astype('int')\n```\n\n## Feature Engineering\n\nLet's transform categorical variables into numerical ones:\n\n```python\n# One-hot encode categorical variables\ndff = pd.concat([df, pd.get_dummies(df['Month'], prefix='Month')], axis=1).drop(['Month'], axis=1)\ndff = pd.concat([dff, pd.get_dummies(dff['VisitorType'], prefix='VisitorType')], axis=1).drop(['VisitorType'], axis=1)\n```\n\n## Model Building with Cross-Validation\n\nLet's implement K-fold cross-validation to evaluate our models:\n\n```python\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\n# Split data\ny = dff['Revenue']\nX = dff.drop(['Revenue'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\n# Scale features\nsc_X = StandardScaler()\nXsc_train = sc_X.fit_transform(X_train)\nXsc_test = sc_X.fit_transform(X_test)\n\n# K-fold cross-validation\ncv = KFold(n_splits=5)\nlogStats = {}\nknnStats = {}\n\nfor ii, (tr, tt) in enumerate(cv.split(X=X_train, y=y_train)):\n    # Logistic Regression\n    lrm = LogisticRegression(C=1.0, solver='lbfgs', max_iter=10000)\n    lrm.fit(X_train.iloc[tr,:], y_train.iloc[tr])\n    lrm_pred = lrm.predict(X_train.iloc[tt,:])\n    y_val = y_train.iloc[tt]\n    \n    print(f'Fold {ii+1} - Logistic Regression:')\n    print(f'Accuracy: {metrics.accuracy_score(y_val, lrm_pred):.4f}')\n    print(f'F1 Score: {metrics.f1_score(y_val, lrm_pred):.4f}')\n    \n    logStats.setdefault('accuracy', []).append(metrics.accuracy_score(y_val, lrm_pred))\n    \n    # K-Nearest Neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(Xsc_train[tr], y_train.iloc[tr])\n    knn_pred = knn.predict(Xsc_train[tt])\n    \n    print(f'Fold {ii+1} - KNN:')\n    print(f'Accuracy: {metrics.accuracy_score(y_val, knn_pred):.4f}')\n    print(f'F1 Score: {metrics.f1_score(y_val, knn_pred):.4f}')\n    print('-' * 40)\n    \n    knnStats.setdefault('accuracy', []).append(metrics.accuracy_score(y_val, knn_pred))\n\n# Average performance\nprint(f'Average Logistic Regression Accuracy: {np.mean(logStats[\"accuracy\"]):.4f}')\nprint(f'Average KNN Accuracy: {np.mean(knnStats[\"accuracy\"]):.4f}')\n```\n\n## Results\n\nLogistic Regression achieved approximately 88% accuracy, with precision around 73% and recall around 38%. The model performs well but could be improved, particularly in terms of recall for the positive class (purchase).\n\n## Conclusion\n\nFeature engineering and proper cross-validation are essential for building robust predictive models for online shopper behavior. The models developed can help e-commerce businesses optimize their websites and marketing strategies to increase conversion rates."
}
